Chapter1 Speed Is a Feature

Faster sites lead to better user engagement.

Faster sites lead to better user retention.

Faster sites lead to higher conversions.

Latency
  The time from the source sending a packet to the destination receiving it

Bandwidth
  Maximum throughput of a logical or physical communication path

Propagation delay
  Amount of time required for a message to travel from the sender to receiver, which is a
  function of distance over speed with which the signal propagates.

Transmission delay
  Amount of time required to push all the packet's bits into the link, which is a function of the
  packet's length and data rate of the link.

Processing delay
  Amount of time required to process the packet header, check for bit-level errors, and determine
  the packet's destination.

Queuing delay
  Amount of time the incoming packet is waiting in the queue until it can be processed.


Each packet traveling over the network will incur many instances of each of these delays. The
farther the distance between the source and destination, the more time it will take to propagate.
The more intermediate routers we encounter along the way, the higher the processing and
transmission delays for each packet. Finally, the higher the load of traffic along the path, the
higher the likelihood of our packet being delayed inside an incoming buffer.


As a result, to improve performance of our applications, we need to architect and optimize our
protocols and networking code with explicit awareness of the limitations of available bandwidth
and the speed of light: we need to reduce round trips, move the data closer to the client, and
build applications that can hide the latency through caching, pre-fetching, and a variety of
similar techniques, as explained in subsequent chapters.


Chapter 2
Building Blocks of TCP

As the heart of the Internet are two protocols, IP and TCP. The IP, or Internet Protocol, is what 
provides the host-to-host routing and addressing, and TCP, or Transimission Control Protocol, is 
what provides the abstraction of a relible network running over an unreliable channel.

Three-Way Handshake
SYN
  Client picks a random sequence number x and sends a SYN packet, witch may also include additional 
  TCP flags and options.

SYN ACK
  Server increments x by one, picks own random sequence number by y, appends its own set of flags 
  and options, and dispatches the response.

ACK 
  Client increments both x and y by one and completes the handshake by dispatching the last ACK 
  packet in the handshake,


Flow Control

Flow control is a mechanism to prevent the sender from overwhelming the receiver with data it may
not be able to process-the receiver may be busy, under heavy load, or may only be willing to
allocate a fixed amount of buffer space.

If you connection to the server ro the client, is unable to make full use of the available 
bandwidth, then checking the interaction of your window sizes is always a good place to start.

Neither the sender nor the receiver knows the available bandwidth at the beginning of a new
connection, and hence need a mechanism to estimate it and also to adapt their speeds to the
continuously changing conditions with the network.

In 1988, Van Jacobson and Michael J.Karels documented several algorithms to address these problems:
show-start, congestion avoidance, fast retransmit, and fast recovery.

The only way to estimate the available capacity between the client and the server is to measure
it by exchanging data, and this is precisely what slow-start is designed to do.

So why is slow-start an important factor to keep in mind when we are building applications for the
browser? Well, HTTP and many other application protocols run over TCP, and no matter the available
bandwidth, every TCP connection must go through the slow-start phase-we cannot use the full
capacity of the link immediately.

Howerver, for many HTTP connections, which are often short and bursty, it is not unusual for the
request to terminate before the maximum window size is reached. As a result, the performance of
many web applications is often limited by the roundtrip time between server and client: slow-start
limits the available bandwidth throughput, which has an adverse effect on the performance of small
transfers.

The implicit assumption in congestion avoidance is that packet loss is indicative of network
congestion: somewhere along the path we have encountered a congested link or a router, which was
force to drop the packet, and hence we need to adjust our window to avoid inducing more packet loss
to avoid overwhelming the network.


Bandwidth-delay product(BDP)
  Product of data link's capacity and its end-to-end delay. The result is the maximum amount of
  unacknowledged data that can be in flight at any point in time.


In fact, packet loss is necessary to get the best performance from TCP! A dropped packet acts as a
feedback mechanism, which allows the receiver and sender to adjust their sending rates to avoid
overwhelming the network, and to minimize latency.

If a packet is lost, then the audio codec can simply insert a minor break in the audio and continue
processing the incoming packets. If the gap is small, the user may not even notice, and waiting for
the lost packet runs the risk of introducing variable pauses in audio output, which would result
in a much worse experience for the user.

The core principles Optimizing TCP:
  TCP three-way handshake introduces a full roundtrip of latency.
  TCP show-start is applied to every new connection.
  TCP flow and congestion control regulate throughput of all connections.
  TCP throughput is regulated by current congestion window size.


TCP connection can have an even greater impact:
  No bit is faster than one that is not sent; send fewer bits
  We can't make the bits travel faster, but we can move the bits closer.
  TCP connection reuse is critical to improve performance.


Performance Checklist
  Upgrade server kernel to latest version(Linux:3.2++).
  Ensure that cwnd size is set to 10.
  Disable show-start after idle.
  Ensure that window scaling is enabled.
  Eliminate redundant data transfers.
  Compress transfered data.
  Position servers closer to the user to reduce roundtrip times.
  Reuse established TCP connections whenever possible.



CHAPTER 3  Building Blocks of UDP

Datagram
  A self-contained, independent entity of data carrying sufficient information to be routed form
  the source to the destination nodes without reliance on earlier exchanges between the nodes and
  the transporting network.

UDP non-services:
  No guarantee of message delivery
    No acknowledgments, ret ransmissions, or timeouts

  No guarantee of order of delivery
    No packet sequence numbers, no recordering, no head-of-line blocking.

  No connection state tracking
    No connection establishment or teardown state machines.

  No congestion control
    No built-in client or network feedback mechanisms.


Assuming the IP address of the STUN server is known(through DNS discovery, or through a manually
specified address), the application first sends a binding request to the STUN server. In turn, the
STUN server replies with a response that contains the public IP address and port of the client as
seen form the public network. This simple workflow addresses serveral problems we encountered in
our earliar discussion:
  The application discovers its public IP and port tuple and is then able to use this information
  as part of its application data when communicating with its peers.

  The outbound binding request to the STUN server establishes NAT routing entries along the path,
  such that the inbound packets arriving at the public IP and port tuple can now find their way
  back to the host application on the internal network.

  The STUN protocol defines a simple mechanism for keeplive pings to keep the NAT routing entries
  from timing out.

Whenever STUN fails, we can use the Traversal Using Relays around NAT(TURN) protocol as a fallback,
which can run over UDP and switch to TCP if all else fails.

They key word in TURN is, of course, "relays." The protocol relies on the presence and availability
of a public relay to shuttle the data between the peers.
  Both clients begin their connections by sending an allocate request to the same TURN server, 
  followed by permissions negotiation.

  Once the negotiation is complete, both peers communicate by sending their data to the TURN server,
  which then relays to the other peers.

Building an effective NAT traversal solution is not for the faint of heart. Thankfully, we can 
learn on Interactive Connectivity Establishment(ICE) protocol to help with this task. ICE is a 
protocol, and a set of methods, that seek to establish the most efficient tunnel between the
participants: direct connection where possible, leveraging STUN negotiation where needed, and
finally fallback to TURN if all else fails.


In practice, if you are building a P2P application over UDP, then you most definitely want to
leverage an existing platform API, or a third-party library that implements ICE, STUN, and TURN for
you.


Unlike TCP, which ships with build-in flow and congestion control and congestion avoidance, UDP
applications must implement these mechanisms on their own. Congestion insensitive UDP applications
can easily overwhelm the network, which can lead to degraded network performance and, in server
case, to network congestion collapse.

Here is a short sample of the recommendations:
  Application must tolerate a wide range of Internet path conditions.

  Application should control rate of transmission.

  Application should perform congestion control over all traffic.

  Application should use bandwidth similar to TCP.

  Application should back off retransmission counters following loss.

  Application should not send datagrams that exceed path MTU.

  Application should handle datagram loss, duplication, and reordering.

  Application should be robust to delivery delays up to 2 minutes.

  Application may use keepalives when needed.




CHAPTER 4
Transport Layer Security

The TLS protocol is designed to provide three essential services to all applications running above
it: encryption, authentication, and data integrity.

  Encryption:
    A mechanism to obfuscate what is send from one computer to another.

  Authentication:
    A mechanism to verify the validity of provided identification material.

  Integrity:
    A mechanism to detect message tampering and forgery.


The ingenious part of this handshake, and the reason TLS works in practice, is its use of public
key cryptography(also known as asymmetric key cryptography), which allows the peers to negotiate
a shared secret key without having to establish any prior knowledge of each other, and to do so
over an unencrypted channel.

Finally, with encryption and authentication in place, the TLS protocol also provides its own
message framing mechanism and signs each message with a message authentication code(MAC). The MAC
algorithm is a one-way cryptographic hash function(effectively a checksum), the keys to which are
negotiated by both connection peers. Whenever a TLS record is sent, a MAC value is generated and
appended for that message, and the receiver is then able to compute and verify the send MAC value
to ensure message integrity and authenticity.

 
As the name implies,, Applicationi Layer Protocol Negotiation(ALPN) is a TLS extension that
introduces support for application negotiation into the TLS handshake thereby eliminating the need
for an extra roundtrip required by the HTTP Upgrade workflow. Specifically, the process if as
follows:
  The client appends a new ProtocolNameList field, containing the list of supported application
  protocols, into the ClientHello message.

  The server inspects the ProtocolNameList field and returns a ProtocolName field indicating
  the selected protocol as part of the ServerHello message.

  The Server Name Indication (SNI) extension was introduced to the TLS protocol, which allows the
  client to indicate the hostname the client is attempting to connect to at the start of the
  handshake. As a result, a web server can inspect the SNI hostname, select the appropriate,
  and continue the handshake.


RLS Session Resumption

The first Session Identifiers(RFC 5246) resumption mechanism was introduced in SSL 2.0, which 
allowed the server to create and send a 32-byte session identifier as part of its "ServerHello"
message during the full TLS negotiation we saw earlier.

Internally, the server could then maintain a cache of session IDs and the negotiated session
parameters for each peer. In turn, the client could then also store the session ID information and
include the ID in the "ClientHello" message for a subsequent session, which servers as an
indication to the server that the client still remembers the negotiated cipher suite and keys from
previous handshake and is able to reuse them. Assuming both the client and the server are able to
find the shared session ID parameters in their respective caches, then an abbreviated handshake can
take place. Ohterwise, a full new session negotiation is required, which will generate a new 
session ID.

In practice, most web applications attempt to establish multiple connections to the same host to
fetch resources in parallel, which makes session resumption a must-have optimization to reduce
latency and computational costs for both sides.

Most modern browsers intentionally wait for the first TLS connection to complete before opening new
connections to the same server: subsequent TLS connections can reuse the SSL session parameters to
avoid the costly handshake.


Session Ticket

Session Tickets removes the requirement for the server to keep per-client session state. Instead,
if the client indicated that it supports Session Tickets, in the last exchange of the full TLS
handshake, the server can include a New Session Ticket record, which includes all of the session
data encrypted with a secret key known only by the server.


The session identifiers and session ticket mechanisms are respectively commonly referred to as 
session caching and stateless resumption mechanisms. The main improvement of stateless resumption
is the removal of the server-side session cache,  which simplifies deployment by requiring that the
client provide the session ticket on every new connection to the server- that is, until the ticket
has expired.


The requirement to perform real-time Online Certificate Status Protocol(OCSP) queries creates
several problems of its own:
  The CA must be able to handle the load of the real-time queries.

  The CA must ensure that the service is up and globally available at all times.

  The client must block on OCSP requests beform proceeding with the navigation.

  Real-time OCSP requests may impair the client's privacy because the CA knows which site the 
  client is visiting.


A typical workflow for delivering application data is as follows:
  Record protocol receives application data.

  Received data is divided into blocks: maximum of 2..14 bytes, or 16KB per record.

  Application data is optionally compressed.

  Message authentication  code (MAC) or HMAC is added.

  Data is encrypted using the negotiated cipher.


Session tickets will be used for clients that support them, and session identifiers as a fallback
for older clients.



HTTP Strict Transport Security is a security policy mechanism that allows the server to declare
access rules to a compliant browser via a simple HTTP header -- e.g. Strict-Transport-Security:
max-age = 31536000. Specifically, it instructs the user-agent to enforce the following rules:
  All requests to the origin should be sent over HTTPS.

  All insecure links and client requests should be automatically converted to HTTPS on the client
  before the request is sent.

  In case of a certificate error, an error message is displayed, and the user is not allowed to
  circumvent the warning.

  max-age specifies the lifetime of the specified HSTS rulest in seconds.

  Optionally the UA can be instructed to remember("pin") the fingerprint of a host in the specified
  certificate chain for future access, effectively limiting the scope of authorities who can
  authenticate the certificate.


A short list to put on the agenda:
  Get best performance from TCP;

  Upgrade TLS libraries to latest release, and build servers against them.

  Enable and configure session caching and stateless resumption.

  Monitor your session caching hit rates and adjust configuration accordingly.

  Terminate TLS sessions closer to the user to minimize roundtrip latencies.

  Configure your TLS record size to fit into a single TCP segment.

  Ensure that your certificate chain does not overflow the initial congestion window.

  Remove unnecessary certificates form your chain; minimize the depth.

  Disable TLS conpression on your server.

  Configure SNI support on your server.

  Configure OCSP stapling on your server.

  Append HTTP Strict Transport Security header.



CHAPTER 5 

Introduce to Wireless Networks

In fact, Claude E. Shannon gave us a exact mathematical model to determine channel capacity,
regardless of the technology in use.
  
  Equation 5-1. Channel capacity is the maximum information rate
    C = BW x log2(1 + S/N)
  C si the channel capacity and is measured in bits per second.
  
  BW is the available bandwidth, and is measured in hertz.
  
  S is signal and N is noise, and they are measured in watts.

Regardless of the name, acronym, or the revision number of the specification, the two fundamental
constraints on achievable data rates are the amount of available bandwidth and the signal prower
between the receiver and the sender.

Finally, it is also worth noting that all frequency ranges offers the same performance. 
Lower-frequency signals travel farther and cover large areas(macrocells), but at the cose of 
requiring larger antennas and having more clients competing for access. On the other hand,
high-frequency signals can transfer more data but won't travel as far, resulting in smaller
coverage areas (microcells) and a requirement for more infrastructure.


In fact, this scenario illustrates two important effects:
  Mear-far problem
    A condition in which a receiver captures a strong signal and thereby makes it impossible for
    the receiver to detect a weaker signal, effectively "crowding out" the weaker signal.

  Cell-breathing
    A condition in which the coverage area, or the distance of the signal, expands and shrinks
    based on the cumulative noise and interference levels.


Modulation is the process of digital-to-analog conversion, and different "modulation alphabets"
can be used to encode the digital singal with different efficiency. The combination of the
alphabet and the symbol rate is what then determines the final throughput of the channel. As a
hands-on example:
  Receiver and sender can process 1000 pulses or symbols per second(1000 baud).

  Each transmitted symbol represents a different bit-sequence, determined by the chosen alphabet.
  (e.g., 2-bit alphabet: 00, 01, 10 11)

  The bit rate of the channel is 1000 baud x 2 bits per symbol, or 2000 bits per second.

Our brief crash course on signal theory can be summed up as follows: the performance of any
wireless network, regardless of the name, acronym, or the revision number, is fundamentally limited
by a small number of well-known parameters. Specifically, the amount of allocated bandwidth and the
signal-to-noise ratio between receiver and sender. Futher, all radio-powered communication is:
  Done over a shared communication medium.

  Regulated to use specific bandwidth frequency ranges.

  Regulated to use specific transmit power rates.

  Subject to continuously changing background noise and interference.

  Subject to technical constraints of the chosen wireless technology.

  subject to constraints of the device: form factor, power,etc.

Needless to say, what you see on the label and what you experience in the real world might be very
different.

Just a few factors that may affect the performance of your wireless network:
  Amount of distance between receiver and sender

  Amount of background noise in current location

  Amount of interference from users in the same network(intra-cell)

  Amount of interference from users in other, nearby networks(inter-cell)

  Amount of available transmit power, both at receiver and sender

  Amount of processing power and the chosen modulation scheme.

The ethernet standard has historically relied on a probabilistic carrier sense multiple access(CSMA)
protocol, which is a complicated name for a simple "listen before you speak" algorithm. In brief,
if you have data to send:
  Check whether anyone else is transmitting.

  If the channel is busy, listen until it is free.

  When the channel is free, transmit data immediately.


WiFi follows a very similar but slightly different model: due to hardware limitations of the radio,
it cannot detect collisions while sending data. Hence, WiFi relies on collision avoidance(CSAM/CA),
where each sender attempts to avoid collisions by transmitting only when the channel is sensed to 
be idle, and then sends its full message frame in its entirety. Once the WiFi frame is send, the
sender waits for an explicit acknowledgment from the receiver before proceeding with the next
transmission.

Measuring and Optimizing WiFi Performance
  WiFi provides no bandwidth or latency guarantees or assignment to its users.

  WiFi provides variable bandwidth based on signal-to-noise in its environment.

  WiFi transmit power is limited to 200mW, and likely less in you region.

  WiFi has a limited amount of spectrum in 2.4 GHz and the newer 5GHz bands.

  WiFi access points overlap in their channel assignment by design.

  WiFi access points and peers complete for access to the same radio channel.

There is no such thing as "typical" WiFi performance. The opearating range will vary based on the
standard, location of the user, used devices, and the local radio environment.


CHAPTER 7
Mobile Networks

In 1991, the first 2G network was launched in Finland base on the emerging GSM standard, which
introduced digital signaling within the radio network. This enabled first circuit-switched mobile
data services, such as text messaging.

The combination of GPRS and earlier voice technologies is often described as 2.5G.

Hence, WiFi offers a small power optimization where the access point broadcasts a delivery
trafficindication message(DTIM) within a periodic beacon frame to indicate that it will be
transmitting data for certain clients immediately after. In turn, the clients can listen for these
DTIM frames as hints for when the radio should be ready to receive, and otherwise the radio can
sleep until the next DTIM transmission. This lowers battery use but adds extra latency.

If all users always remained in the same fixed position, and stayed within reach of a single tower,
then a static routing topology would suffice. However, as we all know, that is simple not the case:
users are mobile and must be migrated from tower to tower, and the migration process should not
interrupt any voice or data traffic. Needless to say, this is a nontrivial problem.

In a nutshell, that is all there is to it. This high-level architecture is effectively the same in
all the different generations of mobile data networks. The names of the logical components may
differ, but fundamentally all mobile networks are subject to the following workflow:
  Data arrives at the external packet gateway, which connects the core network to the public
  Internet.

  A set of routing and packet policies is applied at the packet gateway.

  Data is routed from the public gateway to one or more serving gateways, which act as mobility
  anchors for the devices within the radio network.

  A user database service performs the authentication, billing, provisioning of services, and
  location tracking of each user on the network.

  The radio tower performs the necessary resource assignment and negotiation with the target device
  and then delivers the data over the radio interface.

In summary, a user initiating a new request incurs several different latencies:
  Control-plane latency
    Fixed, on-time latency cost incurred for RRC negotiation and state transitions: < 100ms for 
    idle to active, and < 50 ms for dormant to active.

  User-plane latency
    Fixed cost for every application packet transfered between the device and the radio tower < 5ms.

  Core network latency
    Carrier dependent cost for transporting the packet form the radio tower to the packet gateway:
    in practice, 30-100 ms.

  Internet routing latency
    Variable latency cost between the carrier's packet gateway and the destination address on the
    public Internet.


The core idea behind HetNets is a simple one: instead of relying on just the macro coverage of a
large geographic area, which creates a lot of competition for all users, we can also cover the area
with many small cells, each of which can minimize path loss, require lower transmit power, and
enable better performance for all users.


In other words, at least with respect to this test, LTE offers comparable and better performance
than WiFi, which also shows that improved performance is possible, and all the extra complexity
is paying off! The mobile web doesn't have to be slow. In fact, we have all reasons to believe that
we can and will make it faster.



CHAPTER 8
Optimizing for Mobile Networks

Designing applications for the mobile web requires careful planning and consideration of the
presentation of the content within the constraints of the form factor of the device, the unique
performance properties of the radio interface, and the impact on the battery life. The three are
inextricably linked.


In fact, the physical layers of the radio interface are specifically build to optimize the battery
life against the following constraints:
  Radio use at full power can drain a full battery in a matter of hours.

  Radio power requirements are going up with every wireless generation.

  Radio is often second in power consumption only to the screen.

  Radio use has a nonlinear energy profile with respect to data transfered.


Intermittent network access is a performance anti-pattern on mobile networks. In fact, extending
this same logic yields the following rules:
  Polling is exceptionally expensive on mobile networks; minimize it.

  Where possible, push delivery and notifications should be used.

  Outbound and inbound requests should be coalesced and aggregated.

  Noncritical requests should be deferred until the radio is active.


Whenever there if a need for read-time updates, you should consider the following questions:
  What is the best interval of updates and does it match user expectations?

  Instead of a fixed update interval, can an adaptive strategy be used?

  Can the inbound or outbound requests be aggregated into fewer network calls?

  Can the inbound or outbound requests be deferred until later?


A quick summary of what we have learned about the RPC:
  RRC state machines are different for every wireless standard.

  RPC state machines are managed by the radio network for each dvicce.

  PRC state promotions to high power occur when data must be transfered.

  RPC state demotions to lower power on network-configured timeouts.

  (4G) Latency state transitions can take 10 to 100 milliseconds.

  (4G) HSPA+ state transitions are competitive with LTE.

  (3G) HSPA and CDMA state transitions can take several seconds.

  Every network transfer, no matter the size, inccurs an energy tail.

Do not couple user interactions, user feedback, and network communication.


Your application should remain operational, to the extent possible, when the network is unavailable
or a transient failure happens and should adapt based on request type and specific error:
  Do not cache or attempt to guess the state of the network.

  Dispatch the request, listen for failures, and diagnose what happened.

  Transient errors will happen; plan for them, and use a retry strategy.

  Listen to connection state to anticipate the best request strategy.

  Use a backoff algorithm for request retries; do not spin forever.

  If offline, log and dispatch the request later if possible.

  Leverage HTML5 AppCache and localStorage for offline mode.


Instead, anticipate what your users will need next, download the content ahead of time, and let the
radio idle:
  If you need to fetch a large music or a video file, consider downloading the entire file upfront,
  instead of streaming in chunks.

  Prefetch application content and invest in metrics and statistical models to help identify which
  content is appropriate to download ahead of time.

  Prefetch third-party content, such as ads, ahead of time and add application logic to show and
  update their state when necessary.

  Eliminate unnecessary intermittent transfers.




CHAPTER 9
Brief History of HTTP

Let us quickly recap the features of HTTP 0.9:
  Client-server, request-response protocol.

  ASCII protocol, running over a TCP/IP link.

  Designed to transfer hypertext documents(HTML).

  The connection between server and client is closed after every request.



CHAPTER 10
Primer on Web Performance

The simplest definition of PLT is "the time until the loading spinner stops spinning in the
browser". A more technical definition is time to onload event in the browser, which is an event
fired by the browser once the document and all of its dependent resources have finished loading.

Time and user perception.
------------------------------------------------------------------------------------------------
Delay                User perception
-----------------------------------------------------------------------------------------------
0-100ms              Instant
------------------------------------------------------------------------------------------------
100-300ms            Small perceptible delay
------------------------------------------------------------------------------------------------
300-1000ms           Machine is working
------------------------------------------------------------------------------------------------
1000+ms              Likely mental context switch
------------------------------------------------------------------------------------------------
10000+ms             Task is abandoned
------------------------------------------------------------------------------------------------



The exact list of performed optimizations will differ by browser vendor, but at their core the
optimizations can be grouped into two broad classes:
  Document-aware optimization
    The networking stack is intergrated with the document, CSS, and JavaScript parsing pipelines
    to help identify and prioritize critical network assets, dispatch them early, and get the page
    to an interactive state as soon as possible. This is often done via resource priority
    assignments, lookahead parsing, and similar techniques.


  Speculative optimization
    The browser may learn user navigation patterns over time and perform speculative optimizations
    in an attempt to predict the likely user actions by pre-resolving DNS names, pre-connecting to
    likely hostnames, and so on.


There are four techniques employed by most browders:
  Resource pre-fetching and prioritization
    Document, CSS and JavaScript parsers may communicate extra information to the network stack to
    indicate the relative priority of each resource: blocking resources required for first
    rendering are given high priority, while low-priority requests may be temporarily held back
    in a queue.

  DNS pre-resolve
    Likely hostnames are pre-resolved ahead of time to avoid DNS latency on a future HTTP request.
    A pre-resolve may be triggered through learned navigation history, a use action such as
    hovering over a link, or other signals on the page.

  TCP pre-connect
    Following a DNS resolution, the browser may speculatively open the TCP connection in an
    anticipation of an HTTP request. If it guesses right, it can eliminate another full roundtrip
    of network latency.

  Page pre-rendering
    Some browders allow you to hint the likely next destination and can pre-render the entire page
    in a hidden tab, such that it can be instantly swapped in when the user initiates the 
    navigation


To start, pay close attention to the structure and the delivery of each page:
  Critical resources such as CSS and JavaScript should be discoverable as early as possible in the
  document.

  CSS should be delivered as early as possible to unblock rendering and JavaScript execution.

  Noncritical JavaSCript should be defered to avoid blocking DOM and CSSOM construction.

  The HTML document is parsed incrementally by the parser; hence the document should be periodically
  flushed for best performance.



CHAPTER 11 
HTTP 1.X


networking optimizations:
  Reduce DNS lookups
    Every hostname resolution requires a network roundtrip, imposing latency on the request and
    blocking the request while the lookup is in progress.

  Make fewer HTTP requests
    No request is faster than a request not made: eliminate unnecessary resources on your pages.

  Use a Content Delivery Network
    Locating the data geographically closer to the client can significantly reduce the network
    latency of every TCP connection and improve throughput.

  Add an Expires header and configure ETags
    Relevant resources should be cached to avoid re-requesting the same bytes on each and every 
    page. An Exipres header can be used to specify the cache lifetime of the object, allowing it to
    be retrieved directly from the user's cache and eliminating the HTTP request entirely. Etags
    and last-Modified provide an efficient cache revalidation mechanism - effectively a fingerprint
    or a timestamp of the last update.

  Gzip assets
    All text-based assets should be compressed with Gzip when transfered between the client and the
    server. On average, Gzip will reduce the file size by 60-80%, which makes it one of the simpler
    and high-benefit optimization you can do.

  Avoid HTTP redirects
    HTTP redirects can be extremely costly, especially when they redirect the client to a different
    hostname, which results in additional DNS lookup, TCP connection latency, and so on.


Adding support for HTTP keepalive allows us to eliminate the second TCP three-way handshake, avoid
another round of TCP show-start, and save a full roundtrip of network latency.


In practice, due to lack of multiplexing, HTTP pipelining creates many subtile and undocumented
implications for HTTP servers, intermediaries, and clients:
  A single show response blocks all requests behind it.

  When processing in parallel, servers mush buffer pipelined responses, which may exhaust server
  resources - e.g., what if one of the responses is very large? This exposes an attack vector 
  against the server.

  A failed response may terminate the TCP connection, forcing the client to request all the
  subsequent resources, which may cause duplicate processing.

  Detecting pipelining compatibility reliably, where intermediaries may be present, is a nontrivial
  problem.

  Some intermediaries do not support pipelining and may abort the connection, while others may
  serialize all requests.


A quick checklist for enabling pipelining in your own application:
  Your HTTP client must support pipelining.

  Your HTTP server must support pipelining.

  Your application must handle aborted connections and retries.

  Your application must handle idempotency concerns of aborted requests.

  Your application must protect itself from broken intermediaries.
  

A few considerrations to keep in mind:
  TCP three-way handshake introduces a full roundtrip of latency.

  TCP show-start is applied to every new connection.

  TCP flow and congestion control regulate throughput of all connections.

  TCP throughput is regulated by current congestion window size.

However, how an application uses each new, or established, TCP connection can have an even greater
impact:
  No bit is faster than on bit that is not sent; send fewer bits.

  We can't make the bits travel faster, but we can move the bits closer.

  TCP connection reuse is critical to improve performance.


Optimizing TCP performance pays high dividends, regardless of the type of application, for every
new connection to your servers. A short list to put on the agenda:
  Upgrade server kernel to latest version.

  Ensure that cwnd size is set to 10.

  Disable show-start after idle.

  Ensure that window scaling is enabled.

  Eliminate redundant data transfers.

  Compress transferred data.

  Position servers closer to the user to reduce roundtrip times.

  Reuse established TCP connections whenever possible.


CHAPTER 12
HTTP 2.0

The primary goals for HTTP 2.0 are to reduce latency by enabling full request and response
multiplexing, minimize protocol overhead via efficient compression of HTTP header fields, and add
support for request prioritization and server push.


HTTP 2.0 does not modify the application semantics of HTTP in any way. All the core concepts, such
as HTTP methods, status codes, URIs, and header fields, remain in place. Instead, HTTP 2.0 modifies
how the data is formatted(framed) and transported between the client and server, both of whom
manage the entire process, and hides all the complexity from our applications within the new framing
layer. As a result, all existing applications can be delivered without modification.

key design criteria of the protocol:
  Substantially and measurably improve end-user perceived latency in most cases, over HTTP 1.1 using
  TCP.

  Address the "head of line blocking" problem in HTTP.

  Not require multiple connections to a server to enable parallelism, thus improving its use of TCP,
  especially regarding congestion control.

  Retain the semantics of HTTP 1.1, leveraging existing documentation, including (but not limit to)
  HTTP methods, status codes, URIs, and where appropriate, header fields.

  Clearly define how HTTP 2.0 interacts with HTTP 1.x, especially in intermediaries.

  Clearly identify and new extensibility points and policy for their appropriate use.

new HTTP 2.0 terminology:
  Stream
    A bidrectional flow of bytes within an established connection.

  Message
    A complete sequence of frames that map to a logical message.

  Frame
    The smallest unit of communication in HTTP 2.0, each containing a frame header, which at 
    minimum identifies the stream to which the frame belongs.


The terminology of streams, messages, and frames is essential knowledge for understanding HTTP 2.0:
  All communication is performed with a single TCP connection.

  The Stream is virtual channel within a connection, which carries bidirectional messages. Each
  Stream has a unique integer identifier(1, 2, ..., N).

  The message is logical HTTP message, such as a request, or response, which consists of one or
  more frames.

  The frame is the smallest unit of communication, which carries a specific type of data.

The ability to break down an HTTP message into independent frames, interleave them, and then 
reassemble them on the other end is the single most important enhancement of HTTP 2.0:
  Interleave multiple requests in parallel without blocking on any one.

  Interleave multiple responses in parallel without blocking on any one.

  Use a single single connection to deliver multiple requests and responses in parallel.

  Remove unnecessary HTTP 1.x workarounds from our application code.


The new binary framing layer in HTTP 2.0 resolves the head-of-line blocking problem found in 
HTTP 1.1 and eliminates the need for multiple connections to enable parallel processing and
delivery of requests and responses.

One connection per origin:
  Consistent prioritization between all streams

  Better compression through use of a single compression context

  Improved impact on network congestion due to fewer TCP connections

  Less time in slow-start and faster congestion and loss recovery


HTTP 2.0 provides a simple mechanism for stream and connection flow control:
  Flow control is hop-by-hop, not end-to-end.

  Flow control is based on window update frames: receiver advertises how many bytes it is prepared
  to receive on a stream and for the entire connection.

  Flow control window size is updated by a WINDOW_UPDATE frame, which specifies the stream ID and
  the window size increment value.

  Flow control is directional: receiver may choose to set any window size that it desires for each
  stream and for the entire connection.

  Flow control can be disabled by a receiver, both for an individual stream or for the entire
  connection.


The only difference with HTTP 2.0 is that we can now move this workflow out of the application and
into the HTTP protocol itself, which offers important benefits:
  Pushed resources can be cached by the client.

  Pushed resources can be declined by the client.

  Pushed resources can be reused across different pages.

  Pushed resources can be prioritized by the server. 


To reduce this overhead and improve performance, HTTP 2.0 compresses header metadata:
  Instead of retransmitting the same data on each request and response, HTTP 2.0 uses 
  "header tables" on both the client and server to track and store previously sent key-value pairs.

  Header tables persist for the entire HTTP 2.0 connection and are incrementally updated both by 
  the client and server.

  Each new header key-value pair is either appended to the existing table or replaces a previous
  value in the table.

Once the frame type is known, the remainder of the frame can be interpreted by the parser. The
HTTP 2.0 standard defines the following types:
  DATA            Used to transport HTTP message bodies

  HEADERS         Used to communicate additional header fields for a stream

  PRIORITY        Used to assign or reassign priority of referenced resource

  RST_STREAM      Used to signal abnormal termination of a stream

  SETTINGS        Used to signal configuration data about how two endpoints may communicate

  PUSH_PROMISE    Used to signal a promise to create a stream and serve referenced resource

  PING            Used to measure the roundtrip time and perform "liveness" checks

  GOAWAY          Used to inform the peer to stop creating streams for current

  WINDOW_UPDATE   Used to implement flow control on a per-stream or per-connection basis

  CONTINUATION    Used to continue a sequence of header block fragments


The server determines the order of the frames, and we do not have to worry about the type or content
of each stream. Stream 2 could be a large data transfer or a video stream, but it does not block
the other streams within the shared connection!


CHAPTER 13
Optimizing Application Delivery



