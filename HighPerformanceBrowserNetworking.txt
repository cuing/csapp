Chapter1 Speed Is a Feature

Faster sites lead to better user engagement.

Faster sites lead to better user retention.

Faster sites lead to higher conversions.

Latency
  The time from the source sending a packet to the destination receiving it

Bandwidth
  Maximum throughput of a logical or physical communication path



Propagation delay
  Amount of time required for a message to travel from the sender to receiver, which is a
  function of distance over speed with which the signal propagates.

Transmission delay
  Amount of time required to push all the packet's bits into the link, which is a function of the
  packet's length and data rate of the link.

Processing delay
  Amount of time required to process the packet header, check for bit-level errors, and determine
  the packet's destination.

Queuing delay
  Amount of time the incoming packet is waiting in the queue until it can be processed.


Each packet traveling over the network will incur many instances of each of these delays. The
farther the distance between the source and destination, the more time it will take to propagate.
The more intermediate routers we encounter along the way, the higher the processing and
transmission delays for each packet. Finally, the higher the load of traffic along the path, the
higher the likelihood of our packet being delayed inside an incoming buffer.


As a result, to improve performance of our applications, we need to architect and optimize our
protocols and networking code with explicit awareness of the limitations of available bandwidth
and the speed of light: we need to reduce round trips, move the data closer to the client, and
build applications that can hide the latency through caching, pre-fetching, and a variety of
similar techniques, as explained in subsequent chapters.


Chapter 2
Building Blocks of TCP

As the heart of the Internet are two protocols, IP and TCP. The IP, or Internet Protocol, is what 
provides the host-to-host routing and addressing, and TCP, or Transimission Control Protocol, is 
what provides the abstraction of a relible network running over an unreliable channel.

Three-Way Handshake
SYN
  Client picks a random sequence number x and sends a SYN packet, witch may also include additional 
  TCP flags and options.

SYN ACK
  Server increments x by one, picks own random sequence number by y, appends its own set of flags 
  and options, and dispatches the response.

ACK 
  Client increments both x and y by one and completes the handshake by dispatching the last ACK 
  packet in the handshake,


Flow Control

Flow control is a mechanism to prevent the sender from overwhelming the receiver with data it may
not be able to process-the receiver may be busy, under heavy load, or may only be willing to
allocate a fixed amount of buffer space.

If you connection to the server ro the client, is unable to make full use of the available 
bandwidth, then checking the interaction of your window sizes is always a good place to start.

Neither the sender nor the receiver knows the available bandwidth at the beginning of a new
connection, and hence need a mechanism to estimate it and also to adapt their speeds to the
continuously changing conditions with the network.

In 1988, Van Jacobson and Michael J.Karels documented several algorithms to address these problems:
show-start, congestion avoidance, fast retransmit, and fast recovery.

The only way to estimate the available capacity between the client and the server is to measure
it by exchanging data, and this is precisely what slow-start is designed to do.

So why is slow-start an important factor to keep in mind when we are building applications for the
browser? Well, HTTP and many other application protocols run over TCP, and no matter the available
bandwidth, every TCP connection must go through the slow-start phase-we cannot use the full
capacity of the link immediately.

Howerver, for many HTTP connections, which are often short and bursty, it is not unusual for the
request to terminate before the maximum window size is reached. As a result, the performance of
many web applications is often limited by the roundtrip time between server and client: slow-start
limits the available bandwidth throughput, which has an adverse effect on the performance of small
transfers.

The implicit assumption in congestion avoidance is that packet loss is indicative of network
congestion: somewhere along the path we have encountered a congested link or a router, which was
force to drop the packet, and hence we need to adjust our window to avoid inducing more packet loss
to avoid overwhelming the network.


Bandwidth-delay product(BDP)
  Product of data link's capacity and its end-to-end delay. The result is the maximum amount of
  unacknowledged data that can be in flight at any point in time.


In fact, packet loss is necessary to get the best performance from TCP! A dropped packet acts as a
feedback mechanism, which allows the receiver and sender to adjust their sending rates to avoid
overwhelming the network, and to minimize latency.

If a packet is lost, then the audio codec can simply insert a minor break in the audio and continue
processing the incoming packets. If the gap is small, the user may not even notice, and waiting for
the lost packet runs the risk of introducing variable pauses in audio output, which would result
in a much worse experience for the user.

The core principles Optimizing TCP:
  TCP three-way handshake introduces a full roundtrip of latency.
  TCP show-start is applied to every new connection.
  TCP flow and congestion control regulate throughput of all connections.
  TCP throughput is regulated by current congestion window size.


TCP connection can have an even greater impact:
  No bit is faster than one that is not sent; send fewer bits
  We can't make the bits travel faster, but we can move the bits closer.
  TCP connection reuse is critical to improve performance.


Performance Checklist
  Upgrade server kernel to latest version(Linux:3.2++).
  Ensure that cwnd size is set to 10.
  Disable show-start after idle.
  Ensure that window scaling is enabled.
  Eliminate redundant data transfers.
  Compress transfered data.
  Position servers closer to the user to reduce roundtrip times.
  Reuse established TCP connections whenever possible.



CHAPTER 3  Building Blocks of UDP

Datagram
  A self-contained, independent entity of data carrying sufficient information to be routed form
  the source to the destination nodes without reliance on earlier exchanges between the nodes and
  the transporting network.

UDP non-services:
  No guarantee of message delivery
    No acknowledgments, ret ransmissions, or timeouts

  No guarantee of order of delivery
    No packet sequence numbers, no recordering, no head-of-line blocking.

  No connection state tracking
    No connection establishment or teardown state machines.

  No congestion control
    No built-in client or network feedback mechanisms.


Assuming the IP address of the STUN server is known(through DNS discovery, or through a manually
specified address), the application first sends a binding request to the STUN server. In turn, the
STUN server replies with a response that contains the public IP address and port of the client as
seen form the public network. This simple workflow addresses serveral problems we encountered in
our earliar discussion:
  The application discovers its public IP and port tuple and is then able to use this information
  as part of its application data when communicating with its peers.

  The outbound binding request to the STUN server establishes NAT routing entries along the path,
  such that the inbound packets arriving at the public IP and port tuple can now find their way
  back to the host application on the internal network.

  The STUN protocol defines a simple mechanism for keeplive pings to keep the NAT routing entries
  from timing out.

Whenever STUN fails, we can use the Traversal Using Relays around NAT(TURN) protocol as a fallback,
which can run over UDP and switch to TCP if all else fails.

They key word in TURN is, of course, "relays." The protocol relies on the presence and availability
of a public relay to shuttle the data between the peers.
  Both clients begin their connections by sending an allocate request to the same TURN server, 
  followed by permissions negotiation.

  Once the negotiation is complete, both peers communicate by sending their data to the TURN server,
  which then relays to the other peers.

Building an effective NAT traversal solution is not for the faint of heart. Thankfully, we can 
learn on Interactive Connectivity Establishment(ICE) protocol to help with this task. ICE is a 
protocol, and a set of methods, that seek to establish the most efficient tunnel between the
participants: direct connection where possible, leveraging STUN negotiation where needed, and
finally fallback to TURN if all else fails.


In practice, if you are building a P2P application over UDP, then you most definitely want to
leverage an existing platform API, or a third-party library that implements ICE, STUN, and TURN for
you.


Unlike TCP, which ships with build-in flow and congestion control and congestion avoidance, UDP
applications must implement these mechanisms on their own. Congestion insensitive UDP applications
can easily overwhelm the network, which can lead to degraded network performance and, in server
case, to network congestion collapse.

Here is a short sample of the recommendations:
  Application must tolerate a wide range of Internet path conditions.

  Application should control rate of transmission.

  Application should perform congestion control over all traffic.

  Application should use bandwidth similar to TCP.

  Application should back off retransmission counters following loss.

  Application should not send datagrams that exceed path MTU.

  Application should handle datagram loss, duplication, and reordering.

  Application should be robust to delivery delays up to 2 minutes.

  Application may use keepalives when needed.




CHAPTER 4
Transport Layer Security

The TLS protocol is designed to provide three essential services to all applications running above
it: encryption, authentication, and data integrity.

  Encryption:
    A mechanism to obfuscate what is send from one computer to another.

  Authentication:
    A mechanism to verify the validity of provided identification material.

  Integrity:
    A mechanism to detect message tampering and forgery.


The ingenious part of this handshake, and the reason TLS works in practice, is its use of public
key cryptography(also known as asymmetric key cryptography), which allows the peers to negotiate
a shared secret key without having to establish any prior knowledge of each other, and to do so
over an unencrypted channel.

Finally, with encryption and authentication in place, the TLS protocol also provides its own
message framing mechanism and signs each message with a message authentication code(MAC). The MAC
algorithm is a one-way cryptographic hash function(effectively a checksum), the keys to which are
negotiated by both connection peers. Whenever a TLS record is sent, a MAC value is generated and
appended for that message, and the receiver is then able to compute and verify the send MAC value
to ensure message integrity and authenticity.

 
As the name implies,, Applicationi Layer Protocol Negotiation(ALPN) is a TLS extension that
introduces support for application negotiation into the TLS handshake thereby eliminating the need
for an extra roundtrip required by the HTTP Upgrade workflow. Specifically, the process if as
follows:
  The client appends a new ProtocolNameList field, containing the list of supported application
  protocols, into the ClientHello message.

  The server inspects the ProtocolNameList field and returns a ProtocolName field indicating
  the selected protocol as part of the ServerHello message.

  The Server Name Indication (SNI) extension was introduced to the TLS protocol, which allows the
  client to indicate the hostname the client is attempting to connect to at the start of the
  handshake. As a result, a web server can inspect the SNI hostname, select the appropriate,
  and continue the handshake.


RLS Session Resumption

The first Session Identifiers(RFC 5246) resumption mechanism was introduced in SSL 2.0, which 
allowed the server to create and send a 32-byte session identifier as part of its "ServerHello"
message during the full TLS negotiation we saw earlier.

Internally, the server could then maintain a cache of session IDs and the negotiated session
parameters for each peer. In turn, the client could then also store the session ID information and
include the ID in the "ClientHello" message for a subsequent session, which servers as an
indication to the server that the client still remembers the negotiated cipher suite and keys from
previous handshake and is able to reuse them. Assuming both the client and the server are able to
find the shared session ID parameters in their respective caches, then an abbreviated handshake can
take place. Ohterwise, a full new session negotiation is required, which will generate a new 
session ID.

In practice, most web applications attempt to establish multiple connections to the same host to
fetch resources in parallel, which makes session resumption a must-have optimization to reduce
latency and computational costs for both sides.

Most modern browsers intentionally wait for the first TLS connection to complete before opening new
connections to the same server: subsequent TLS connections can reuse the SSL session parameters to
avoid the costly handshake.


Session Ticket

Session Tickets removes the requirement for the server to keep per-client session state. Instead,
if the client indicated that it supports Session Tickets, in the last exchange of the full TLS
handshake, the server can include a New Session Ticket record, which includes all of the session
data encrypted with a secret key known only by the server.


The session identifiers and session ticket mechanisms are respectively commonly referred to as 
session caching and stateless resumption mechanisms. The main improvement of stateless resumption
is the removal of the server-side session cache,  which simplifies deployment by requiring that the
client provide the session ticket on every new connection to the server- that is, until the ticket
has expired.


The requirement to perform real-time Online Certificate Status Protocol(OCSP) queries creates
several problems of its own:
  The CA must be able to handle the load of the real-time queries.

  The CA must ensure that the service is up and globally available at all times.

  The client must block on OCSP requests beform proceeding with the navigation.

  Real-time OCSP requests may impair the client's privacy because the CA knows which site the 
  client is visiting.


A typical workflow for delivering application data is as follows:
  Record protocol receives application data.

  Received data is divided into blocks: maximum of 2..14 bytes, or 16KB per record.

  Application data is optionally compressed.

  Message authentication  code (MAC) or HMAC is added.

  Data is encrypted using the negotiated cipher.


Session tickets will be used for clients that support them, and session identifiers as a fallback
for older clients.



HTTP Strict Transport Security is a security policy mechanism that allows the server to declare
access rules to a compliant browser via a simple HTTP header -- e.g. Strict-Transport-Security:
max-age = 31536000. Specifically, it instructs the user-agent to enforce the following rules:
  All requests to the origin should be sent over HTTPS.

  All insecure links and client requests should be automatically converted to HTTPS on the client
  before the request is sent.

  In case of a certificate error, an error message is displayed, and the user is not allowed to
  circumvent the warning.

  max-age specifies the lifetime of the specified HSTS rulest in seconds.

  Optionally the UA can be instructed to remember("pin") the fingerprint of a host in the specified
  certificate chain for future access, effectively limiting the scope of authorities who can
  authenticate the certificate.


A short list to put on the agenda:
  Get best performance from TCP;

  Upgrade TLS libraries to latest release, and build servers against them.

  Enable and configure session caching and stateless resumption.

  Monitor your session caching hit rates and adjust configuration accordingly.

  Terminate TLS sessions closer to the user to minimize roundtrip latencies.

  Configure your TLS record size to fit into a single TCP segment.

  Ensure that your certificate chain does not overflow the initial congestion window.

  Remove unnecessary certificates form your chain; minimize the depth.

  Disable TLS conpression on your server.

  Configure SNI support on your server.

  Configure OCSP stapling on your server.

  Append HTTP Strict Transport Security header.



CHAPTER 5 

Introduce to Wireless Networks

In fact, Claude E. Shannon gave us a exact mathematical model to determine channel capacity,
regardless of the technology in use.
  
  Equation 5-1. Channel capacity is the maximum information rate
    C = BW x log2(1 + S/N)
  C si the channel capacity and is measured in bits per second.
  
  BW is the available bandwidth, and is measured in hertz.
  
  S is signal and N is noise, and they are measured in watts.

Regardless of the name, acronym, or the revision number of the specification, the two fundamental
constraints on achievable data rates are the amount of available bandwidth and the signal prower
between the receiver and the sender.

Finally, it is also worth noting that all frequency ranges offers the same performance. 
Lower-frequency signals travel farther and cover large areas(macrocells), but at the cose of 
requiring larger antennas and having more clients competing for access. On the other hand,
high-frequency signals can transfer more data but won't travel as far, resulting in smaller
coverage areas (microcells) and a requirement for more infrastructure.


In fact, this scenario illustrates two important effects:
  Mear-far problem
    A condition in which a receiver captures a strong signal and thereby makes it impossible for
    the receiver to detect a weaker signal, effectively "crowding out" the weaker signal.

  Cell-breathing
    A condition in which the coverage area, or the distance of the signal, expands and shrinks
    based on the cumulative noise and interference levels.


Modulation is the process of digital-to-analog conversion, and different "modulation alphabets"
can be used to encode the digital singal with different efficiency. The combination of the
alphabet and the symbol rate is what then determines the final throughput of the channel. As a
hands-on example:
  Receiver and sender can process 1000 pulses or symbols per second(1000 baud).

  Each transmitted symbol represents a different bit-sequence, determined by the chosen alphabet.
  (e.g., 2-bit alphabet: 00, 01, 10 11)

  The bit rate of the channel is 1000 baud x 2 bits per symbol, or 2000 bits per second.

Our brief crash course on signal theory can be summed up as follows: the performance of any
wireless network, regardless of the name, acronym, or the revision number, is fundamentally limited
by a small number of well-known parameters. Specifically, the amount of allocated bandwidth and the
signal-to-noise ratio between receiver and sender. Futher, all radio-powered communication is:
  Done over a shared communication medium.

  Regulated to use specific bandwidth frequency ranges.

  Regulated to use specific transmit power rates.

  Subject to continuously changing background noise and interference.

  Subject to technical constraints of the chosen wireless technology.

  subject to constraints of the device: form factor, power,etc.

Needless to say, what you see on the label and what you experience in the real world might be very
different.

Just a few factors that may affect the performance of your wireless network:
  Amount of distance between receiver and sender

  Amount of background noise in current location

  Amount of interference from users in the same network(intra-cell)

  Amount of interference from users in other, nearby networks(inter-cell)

  Amount of available transmit power, both at receiver and sender

  Amount of processing power and the chosen modulation scheme.

The ethernet standard has historically relied on a probabilistic carrier sense multiple access(CSMA)
protocol, which is a complicated name for a simple "listen before you speak" algorithm. In brief,
if you have data to send:
  Check whether anyone else is transmitting.

  If the channel is busy, listen until it is free.

  When the channel is free, transmit data immediately.


WiFi follows a very similar but slightly different model: due to hardware limitations of the radio,
it cannot detect collisions while sending data. Hence, WiFi relies on collision avoidance(CSAM/CA),
where each sender attempts to avoid collisions by transmitting only when the channel is sensed to 
be idle, and then sends its full message frame in its entirety. Once the WiFi frame is send, the
sender waits for an explicit acknowledgment from the receiver before proceeding with the next
transmission.