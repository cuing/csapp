Chapter1 Speed Is a Feature

Faster sites lead to better user engagement.

Faster sites lead to better user retention.

Faster sites lead to higher conversions.

Latency
  The time from the source sending a packet to the destination receiving it

Bandwidth
  Maximum throughput of a logical or physical communication path



Propagation delay
  Amount of time required for a message to travel from the sender to receiver, which is a
  function of distance over speed with which the signal propagates.

Transmission delay
  Amount of time required to push all the packet's bits into the link, which is a function of the
  packet's length and data rate of the link.

Processing delay
  Amount of time required to process the packet header, check for bit-level errors, and determine
  the packet's destination.

Queuing delay
  Amount of time the incoming packet is waiting in the queue until it can be processed.


Each packet traveling over the network will incur many instances of each of these delays. The
farther the distance between the source and destination, the more time it will take to propagate.
The more intermediate routers we encounter along the way, the higher the processing and
transmission delays for each packet. Finally, the higher the load of traffic along the path, the
higher the likelihood of our packet being delayed inside an incoming buffer.


As a result, to improve performance of our applications, we need to architect and optimize our
protocols and networking code with explicit awareness of the limitations of available bandwidth
and the speed of light: we need to reduce round trips, move the data closer to the client, and
build applications that can hide the latency through caching, pre-fetching, and a variety of
similar techniques, as explained in subsequent chapters.



Flow Control

Flow control is a mechanism to prevent the sender from overwhelming the receiver with data it may
not be able to process-the receiver may be busy, under heavy load, or may only be willing to
allocate a fixed amount of buffer space.

If you connection to the server ro the client, is unable to make full use of the available 
bandwidth, then checking the interaction of your window sizes is always a good place to start.

Neither the sender nor the receiver knows the available bandwidth at the beginning of a new
connection, and hence need a mechanism to estimate it and also to adapt their speeds to the
continuously changing conditions with the network.

In 1988, Van Jacobson and Michael J.Karels documented several algorithms to address these problems:
show-start, congestion avoidance, fast retransmit, and fast recovery.

The only way to estimate the available capacity between the client and the server is to measure
it by exchanging data, and this is precisely what slow-start is designed to do.

So why is slow-start an important factor to keep in mind when we are building applications for the
browser? Well, HTTP and many other application protocols run over TCP, and no matter the available
bandwidth, every TCP connection must go through the slow-start phase-we cannot use the full
capacity of the link immediately.

Howerver, for many HTTP connections, which are often short and bursty, it is not unusual for the
request to terminate before the maximum window size is reached. As a result, the performance of
many web applications is often limited by the roundtrip time between server and client: slow-start
limits the available bandwidth throughput, which has an adverse effect on the performance of small
transfers.

The implicit assumption in congestion avoidance is that packet loss is indicative of network
congestion: somewhere along the path we have encountered a congested link or a router, which was
force to drop the packet, and hence we need to adjust our window to avoid inducing more packet loss
to avoid overwhelming the network.


Bandwidth-delay product(BDP)
  Product of data link's capacity and its end-to-end delay. The result is the maximum amount of
  unacknowledged data that can be in flight at any point in time.


In fact, packet loss is necessary to get the best performance from TCP! A dropped packet acts as a
feedback mechanism, which allows the receiver and sender to adjust their sending rates to avoid
overwhelming the network, and to minimize latency.

If a packet is lost, then the audio codec can simply insert a minor break in the audio and continue
processing the incoming packets. If the gap is small, the user may not even notice, and waiting for
the lost packet runs the risk of introducing variable pauses in audio output, which would result
in a much worse experience for the user.

The core principles Optimizing TCP:
  TCP three-way handshake introduces a full roundtrip of latency.
  TCP show-start is applied to every new connection.
  TCP flow and congestion control regulate throughput of all connections.
  TCP throughput is regulated by current congestion window size.


TCP connection can have an even greater impact:
  No bit is faster than one that is not sent; send fewer bits
  We can't make the bits travel faster, but we can move the bits closer.
  TCP connection reuse is critical to improve performance.


Performance Checklist
  Upgrade server kernel to latest version(Linux:3.2++).
  Ensure that cwnd size is set to 10.
  Disable show-start after idle.
  Ensure that window scaling is enabled.
  Eliminate redundant data transfers.
  Compress transfered data.
  Position servers closer to the user to reduce roundtrip times.
  Reuse established TCP connections whenever possible.



CHAPTER 3  Building Blocks of UDP

Datagram
  A self-contained, independent entity of data carrying sufficient information to be routed form
  the source to the destination nodes without reliance on earlier exchanges between the nodes and
  the transporting network.

UDP non-services:
  No guarantee of message delivery
    No acknowledgments, ret ransmissions, or timeouts

  No guarantee of order of delivery
    No packet sequence numbers, no recordering, no head-of-line blocking.

  No connection state tracking
    No connection establishment or teardown state machines.

  No congestion control
    No built-in client or network feedback mechanisms.


Assuming the IP address of the STUN server is known(through DNS discovery, or through a manually
specified address), the application first sends a binding request to the STUN server. In turn, the
STUN server replies with a response that contains the public IP address and port of the client as
seen form the public network. This simple workflow addresses serveral problems we encountered in
our earliar discussion:
  The application discovers its public IP and port tuple and is then able to use this information
  as part of its application data when communicating with its peers.

  The outbound binding request to the STUN server establishes NAT routing entries along the path,
  such that the inbound packets arriving at the public IP and port tuple can now find their way
  back to the host application on the internal network.

  The STUN protocol defines a simple mechanism for keeplive pings to keep the NAT routing entries
  from timing out.

Whenever STUN fails, we can use the Traversal Using Relays around NAT(TURN) protocol as a fallback,
which can run over UDP and switch to TCP if all else fails.

They key word in TURN is, of course, "relays." The protocol relies on the presence and availability
of a public relay to shuttle the data between the peers.
  Both clients begin their connections by sending an allocate request to the same TURN server, 
  followed by permissions negotiation.

  Once the negotiation is complete, both peers communicate by sending their data to the TURN server,
  which then relays to the other peers.

Building an effective NAT traversal solution is not for the faint of heart. Thankfully, we can 
learn on Interactive Connectivity Establishment(ICE) protocol to help with this task. ICE is a 
protocol, and a set of methods, that seek to establish the most efficient tunnel between the
participants: direct connection where possible, leveraging STUN negotiation where needed, and
finally fallback to TURN if all else fails.


In practice, if you are building a P2P application over UDP, then you most definitely want to
leverage an existing platform API, or a third-party library that implements ICE, STUN, and TURN for
you.


Unlike TCP, which ships with build-in flow and congestion control and congestion avoidance, UDP
applications must implement these mechanisms on their own. Congestion insensitive UDP applications
can easily overwhelm the network, which can lead to degraded network performance and, in server
case, to network congestion collapse.

Here is a short sample of the recommendations:
  Application must tolerate a wide range of Internet path conditions.

  Application should control rate of transmission.

  Application should perform congestion control over all traffic.

  Application should use bandwidth similar to TCP.

  Application should back off retransmission counters following loss.

  Application should not send datagrams that exceed path MTU.

  Application should handle datagram loss, duplication, and reordering.

  Application should be robust to delivery delays up to 2 minutes.

  Application may use keepalives when needed.




CHAPTER 4
Transport Layer Security

The TLS protocol is designed to provide three essential services to all applications running above
it: encryption, authentication, and data integrity.

  Encryption:
    A mechanism to obfuscate what is send from one computer to another.

  Authentication:
    A mechanism to verify the validity of provided identification material.

  Integrity:
    A mechanism to detect message tampering and forgery.


The ingenious part of this handshake, and the reason TLS works in practice, is its use of public
key cryptography(also known as asymmetric key cryptography), which allows the peers to negotiate
a shared secret key without having to establish any prior knowledge of each other, and to do so
over an unencrypted channel.

Finally, with encryption and authentication in place, the TLS protocol also provides its own
message framing mechanism and signs each message with a message authentication code(MAC). The MAC
algorithm is a one-way cryptographic hash function(effectively a checksum), the keys to which are
negotiated by both connection peers. Whenever a TLS record is sent, a MAC value is generated and
appended for that message, and the receiver is then able to compute and verify the send MAC value
to ensure message integrity and authenticity.

 
As the name implies,, Applicationi Layer Protocol Negotiation(ALPN) is a TLS extension that
introduces support for application negotiation into the TLS handshake thereby eliminating the need
for an extra roundtrip required by the HTTP Upgrade workflow. Specifically, the process if as
follows:
  The client appends a new ProtocolNameList field, containing the list of supported application
  protocols, into the ClientHello message.

  The server inspects the ProtocolNameList field and returns a ProtocolName field indicating
  the selected protocol as part of the ServerHello message.

  The Server Name Indication (SNI) extension was introduced to the TLS protocol, which allows the
  client to indicate the hostname the client is attempting to connect to at the start of the
  handshake. As a result, a web server can inspect the SNI hostname, select the appropriate,
  and continue the handshake.


RLS Session Resumption

The first Session Identifiers(RFC 5246) resumption mechanism was introduced in SSL 2.0, which 
allowed the server to create and send a 32-byte session identifier as part of its "ServerHello"
message during the full TLS negotiation we saw earlier.

Internally, the server could then maintain a cache of session IDs and the negotiated session
parameters for each peer. In turn, the client could then also store the session ID information and
include the ID in the "ClientHello" message for a subsequent session, which servers as an
indication to the server that the client still remembers the negotiated cipher suite and keys from
previous handshake and is able to reuse them. Assuming both the client and the server are able to
find the shared session ID parameters in their respective caches, then an abbreviated handshake can
take place. Ohterwise, a full new session negotiation is required, which will generate a new 
session ID.

In practice, most web applications attempt to establish multiple connections to the same host to
fetch resources in parallel, which makes session resumption a must-have optimization to reduce
latency and computational costs for both sides.

Most modern browsers intentionally wait for the first TLS connection to complete before opening new
connections to the same server: subsequent TLS connections can reuse the SSL session parameters to
avoid the costly handshake.


Session Ticket

Session Tickets removes the requirement for the server to keep per-client session state. Instead,
if the client indicated that it supports Session Tickets, in the last exchange of the full TLS
handshake, the server can include a New Session Ticket record, which includes all of the session
data encrypted with a secret key known only by the server.


The session identifiers and session ticket mechanisms are respectively commonly referred to as 
session caching and stateless resumption mechanisms. The main improvement of stateless resumption
is the removal of the server-side session cache,  which simplifies deployment by requiring that the
client provide the session ticket on every new connection to the server- that is, until the ticket
has expired.


The requirement to perform real-time Online Certificate Status Protocol(OCSP) queries creates
several problems of its own:
  The CA must be able to handle the load of the real-time queries.

  The CA must ensure that the service is up and globally available at all times.

  The client must block on OCSP requests beform proceeding with the navigation.

  Real-time OCSP requests may impair the client's privacy because the CA knows which site the 
  client is visiting.